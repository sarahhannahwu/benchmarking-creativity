{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import cohen_kappa_score, matthews_corrcoef, confusion_matrix\n",
    "\n",
    "def calculate_alignment_metrics(human_decisions, ai_decisions):\n",
    "    \"\"\"\n",
    "    Calculate various metrics to measure preference alignment between humans and AI.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    human_decisions : array-like\n",
    "        Binary array (0 or 1) representing human decisions to save shapes\n",
    "    ai_decisions : array-like\n",
    "        Binary array (0 or 1) representing AI decisions to save shapes\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing various alignment metrics\n",
    "    \"\"\"\n",
    "    # Simple agreement percentage\n",
    "    agreement = np.mean(human_decisions == ai_decisions) * 100\n",
    "    \n",
    "    # Calculate Cohen's Kappa (accounts for agreement by chance)\n",
    "    kappa = cohen_kappa_score(human_decisions, ai_decisions)\n",
    "    \n",
    "    # Calculate Matthews Correlation Coefficient (MCC)\n",
    "    mcc = matthews_corrcoef(human_decisions, ai_decisions)\n",
    "    \n",
    "    # Generate confusion matrix for calculating other metrics\n",
    "    tn, fp, fn, tp = confusion_matrix(human_decisions, ai_decisions).ravel()\n",
    "    \n",
    "    # Calculate precision, recall, F1 score\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Calculate balanced accuracy\n",
    "    sensitivity = recall\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    balanced_accuracy = (sensitivity + specificity) / 2\n",
    "    \n",
    "    # Conditional probability metrics\n",
    "    # P(AI=1|Human=1) - how often AI saves when Human saves\n",
    "    p_ai_given_human = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    \n",
    "    # P(Human=1|AI=1) - how often Human saves when AI saves\n",
    "    p_human_given_ai = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    \n",
    "    # Asymmetric similarity - how well AI captures human preferences\n",
    "    # Jaccard similarity = |intersection| / |union|\n",
    "    jaccard = tp / (tp + fp + fn) if (tp + fp + fn) > 0 else 0\n",
    "    \n",
    "    # Dice coefficient = 2*|intersection| / (|A| + |B|)\n",
    "    dice = 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0\n",
    "    \n",
    "    # Information theory metrics\n",
    "    # Calculate joint and marginal probabilities\n",
    "    n_samples = len(human_decisions)\n",
    "    p_h1_a1 = tp / n_samples\n",
    "    p_h0_a1 = fp / n_samples\n",
    "    p_h1_a0 = fn / n_samples\n",
    "    p_h0_a0 = tn / n_samples\n",
    "    \n",
    "    p_h1 = (tp + fn) / n_samples  # P(Human=1)\n",
    "    p_h0 = (tn + fp) / n_samples  # P(Human=0)\n",
    "    p_a1 = (tp + fp) / n_samples  # P(AI=1)\n",
    "    p_a0 = (tn + fn) / n_samples  # P(AI=0)\n",
    "    \n",
    "    # Calculate mutual information\n",
    "    mi = 0\n",
    "    if p_h1_a1 > 0: mi += p_h1_a1 * np.log2(p_h1_a1 / (p_h1 * p_a1))\n",
    "    if p_h0_a1 > 0: mi += p_h0_a1 * np.log2(p_h0_a1 / (p_h0 * p_a1))\n",
    "    if p_h1_a0 > 0: mi += p_h1_a0 * np.log2(p_h1_a0 / (p_h1 * p_a0))\n",
    "    if p_h0_a0 > 0: mi += p_h0_a0 * np.log2(p_h0_a0 / (p_h0 * p_a0))\n",
    "    \n",
    "    # Calculate entropies\n",
    "    h_entropy = -p_h1 * np.log2(p_h1) if p_h1 > 0 else 0\n",
    "    h_entropy += -p_h0 * np.log2(p_h0) if p_h0 > 0 else 0\n",
    "    \n",
    "    a_entropy = -p_a1 * np.log2(p_a1) if p_a1 > 0 else 0\n",
    "    a_entropy += -p_a0 * np.log2(p_a0) if p_a0 > 0 else 0\n",
    "    \n",
    "    # Calculate normalized mutual information\n",
    "    nmi = mi / max(h_entropy, a_entropy) if max(h_entropy, a_entropy) > 0 else 0\n",
    "    \n",
    "    # Return all metrics in a dictionary\n",
    "    return {\n",
    "        \"Simple Agreement (%)\": agreement,\n",
    "        \"Cohen's Kappa\": kappa,\n",
    "        \"Matthews Correlation Coefficient\": mcc,\n",
    "        \"F1 Score\": f1,\n",
    "        \"Precision (AI alignment when Human saves)\": precision,\n",
    "        \"Recall (Human preferences captured by AI)\": recall,\n",
    "        \"Balanced Accuracy\": balanced_accuracy,\n",
    "        \"P(AI saves | Human saves)\": p_ai_given_human,\n",
    "        \"P(Human saves | AI saves)\": p_human_given_ai,\n",
    "        \"Jaccard Similarity\": jaccard,\n",
    "        \"Dice Coefficient\": dice,\n",
    "        \"Mutual Information\": mi,\n",
    "        \"Normalized Mutual Information\": nmi,\n",
    "        \"Confusion Matrix\": {\n",
    "            \"True Negatives\": tn,  # Both decided not to save\n",
    "            \"False Positives\": fp,  # AI saved but Human didn't\n",
    "            \"False Negatives\": fn,  # Human saved but AI didn't\n",
    "            \"True Positives\": tp,   # Both decided to save\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example data (replace with your actual data)\n",
    "    data = pd.DataFrame({\n",
    "        'shape_id': range(1, 101),\n",
    "        'human_save': np.random.randint(0, 2, 100),\n",
    "        'ai_save': np.random.randint(0, 2, 100)\n",
    "    })\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = calculate_alignment_metrics(data['human_save'], data['ai_save'])\n",
    "    \n",
    "    # Print metrics\n",
    "    for metric, value in metrics.items():\n",
    "        if isinstance(value, dict):\n",
    "            print(f\"{metric}:\")\n",
    "            for submetric, subvalue in value.items():\n",
    "                print(f\"  {submetric}: {subvalue}\")\n",
    "        else:\n",
    "            print(f\"{metric}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmarking-creativity-nbm55LEq-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
